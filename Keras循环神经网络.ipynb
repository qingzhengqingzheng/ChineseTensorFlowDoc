{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Euh3ztsYcpt8"
   },
   "source": [
    "# Keras的循环神经网络（RNN）\n",
    "\n",
    "### 介绍\n",
    "\n",
    "循环神经网络（RNN）是一类神经网络，对于建模序列数据（例如时间序列或自然语言）非常有力。\n",
    "\n",
    "从原理上看，RNN层使用`for`循环在序列的时间步上进行迭代，同时维护内部状态，该状态对已执行的时间步的信息进行编码。\n",
    "\n",
    "Keras RNN API的设计重点是：\n",
    "\n",
    "**易于使用**：内置的[`keras.layers.RNN`]，[`keras.layers.LSTM`]，[`keras.layers.GRU`]层使你能够快速构建循环模型，而不必进行困难的配置选择。\n",
    "\n",
    "**易于定制**：你还可以使用自定义行为定义自己的RNN单元层（`for`循环的内部），并将其与通用`keras.layers.RNN`层（`for`循环本身）一起使用。这使你能够以最少的代码灵活且快速地原型化不同的研究思路。\n",
    "\n",
    "### 引入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O3A4UqrEciP8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "miOxVoKQdkLW"
   },
   "source": [
    "### 内置RNN层：一个简单的示例\n",
    "\n",
    "Keras中有三个内置的RNN层：\n",
    "\n",
    "1. [`keras.layers.SimpleRNN`]，一个完全连接的RNN，它将上一个时间步的输出馈送到下一个时间步。\n",
    "\n",
    "2. [`keras.layers.GRU`]，在[Cho et al.,2014](https://arxiv.org/abs/1406.1078)中首先被提出 。\n",
    "\n",
    "3. [`keras.layers.LSTM`]，在[Hochreiter & Schmidhuber, 1997.](https://www.bioinf.jku.at/publications/older/2604.pdf)中首先被提出 。\n",
    "\n",
    "2015年初，Keras拥有LSTM和GRU的第一个可重用的开源Python实现。\n",
    "\n",
    "下面是一个简单的`Sequential`模型示例，该模型处理整数序列，将每个整数嵌入到64维向量中，然后使用`LSTM`层处理向量序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "WzfVGKuBdo8p",
    "outputId": "86bcaf80-bfb8-4fcf-dcd6-e8eaeaf88ffb"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "# 添加一个输入单词量大小为1000，输出嵌入尺寸为64的嵌入层\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# 添加一个128个内部单元的LSTM层\n",
    "model.add(layers.LSTM(128))\n",
    "\n",
    "# 添加一个10个内部单元的密度层\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "svsrz_2hdquq"
   },
   "source": [
    "内置RNN支持许多有用的功能：\n",
    "\n",
    "+ 通过`dropout`和`recurrent_dropout`参数进行循环dropout\n",
    "+ 可以通过`go_backwards`参数反向处理输入序列\n",
    "+ 通过`unroll`参数进行循环展开（在CPU上处理短序列时可能更快）\n",
    "+ ...和更多。\n",
    "\n",
    "有关更多信息，请参见[RNN API文档](https://keras.io/api/layers/recurrent_layers/)。\n",
    "\n",
    "### 输出和状态\n",
    "\n",
    "默认情况下，RNN层的输出包含每个样本对应的一个向量。该向量是最后一个时间步相对应的RNN单元输出，其中包含有关整个输入序列的信息。此输出的形状为`(batch_size, units)`，其中`units`对应于传递给层构造函数的`units`参数。\n",
    "\n",
    "如果你设置`return_sequences=True`，则RNN层还可以返回每个样本的整个输出序列（每个样本的每个时间步的一个向量），此输出的形状是`(batch_size, timesteps, units)` 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "id": "F61xsqIedvyb",
    "outputId": "909cd8d4-621c-414a-840f-249e02ac49ff"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "model.add(layers.Embedding(input_dim=1000, output_dim=64))\n",
    "\n",
    "# GRU的输出将是形状为（batch_size，timesteps，256）的3D张量\n",
    "model.add(layers.GRU(256, return_sequences=True))\n",
    "\n",
    "# SimpleRNN的输出将是形状为（batch_size，128）的2D张量\n",
    "model.add(layers.SimpleRNN(128))\n",
    "\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0P6AfGPFdxoS"
   },
   "source": [
    "此外，RNN层可以返回其最终的内部状态，返回的状态可用于稍后恢复RNN执行或[初始化另一个RNN](https://arxiv.org/abs/1409.3215)，此设置通常用于编码器-解码器的Seq2Seq模型，其中编码器的最终状态用作解码器的初始状态。\n",
    "\n",
    "要将RNN层配置为返回其内部状态，需要在创建层时将`return_state`参数设置为`True`。值得注意的是，`LSTM`具有2个状态张量，但`GRU`仅具有1个。\n",
    "\n",
    "要配置层的初始状态，只需使用关键字参数`initial_state`来调用层即可。请注意，状态的形状需要与层的单元大小匹配，如以下示例所示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "colab_type": "code",
    "id": "BIghsv75dzQH",
    "outputId": "448fe30c-79e6-4abf-8bfe-77f93ef7d6f4"
   },
   "outputs": [],
   "source": [
    "encoder_vocab = 1000\n",
    "decoder_vocab = 2000\n",
    "\n",
    "encoder_input = layers.Input(shape=(None,))\n",
    "encoder_embedded = layers.Embedding(input_dim=encoder_vocab, output_dim=64)(\n",
    "    encoder_input\n",
    ")\n",
    "\n",
    "# 返回除输出外的状态\n",
    "output, state_h, state_c = layers.LSTM(64, return_state=True, name=\"encoder\")(\n",
    "    encoder_embedded\n",
    ")\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "decoder_input = layers.Input(shape=(None,))\n",
    "decoder_embedded = layers.Embedding(input_dim=decoder_vocab, output_dim=64)(\n",
    "    decoder_input\n",
    ")\n",
    "\n",
    "# 将这两种状态作为初始状态传递到新的LSTM层\n",
    "decoder_output = layers.LSTM(64, name=\"decoder\")(\n",
    "    decoder_embedded, initial_state=encoder_state\n",
    ")\n",
    "output = layers.Dense(10)(decoder_output)\n",
    "\n",
    "model = keras.Model([encoder_input, decoder_input], output)\n",
    "model.summary()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XC96aaRMd0w5"
   },
   "source": [
    "### RNN层和RNN单元\n",
    "\n",
    "除了内置的RNN层之外，RNN API还提供了单元级API。与处理整批量输入序列的RNN层不同，RNN单元仅处理单个时间步。\n",
    "\n",
    "该单元在RNN层的`for`循环的内部。在[`keras.layers.RNN`]层中包装一个单元，可以为你提供能够处理批量序列的层，例如RNN(LSTMCell(10)) 。\n",
    "\n",
    "从数学角度来看，`RNN(LSTMCell(10))`产生与`LSTM(10)`相同的结果。实际上，在TF v1.x中该层的实现只是创建相应的RNN单元并将其包装在RNN层中。但是，使用内置的`GRU`和`LSTM`层可以使用CuDNN，您可能会看到更好的性能。\n",
    "\n",
    "内置三个RNN单元，每个单元对应于匹配的RNN层。\n",
    "\n",
    "+ [`keras.layers.SimpleRNNCell`]对应于SimpleRNN层。\n",
    "\n",
    "+ [`keras.layers.GRUCell`]对应于GRU层。\n",
    "\n",
    "+ [`keras.layers.LSTMCell`]对应于LSTM层。\n",
    "\n",
    "单元抽象以及通用的`keras.layers.RNN`类使得实现自定义RNN体系结构变得非常容易。\n",
    "\n",
    "### 跨批状态\n",
    "\n",
    "当处理非常长的序列（可能是无限的）时，你可能需要使用跨批状态的模式。\n",
    "\n",
    "通常，每次看到新批次时，都会重置RNN层的内部状态（即，假定该层看到的每个样本都独立于过去的），该层仅在处理给定样本时保持状态。\n",
    "\n",
    "如果你有很长的序列，则将它们分成较短的序列，然后将这些较短的序列依次传入RNN层而不重置该层的状态将非常有用。这样，即使一次只看到一个子序列，该层也可以保留有关整个序列的信息。\n",
    "\n",
    "你可以通过在构造函数中设置`stateful=True`来实现。\n",
    "\n",
    "如果有一个序列`s = [t0, t1, ... t1546, t1547]` ，则将其拆分为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jjx4SDsSd-cX"
   },
   "outputs": [],
   "source": [
    "s1 = [t0, t1, ... t100]\n",
    "s2 = [t101, ... t201]\n",
    "...\n",
    "s16 = [t1501, ... t1547]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJx4ujrRd_ST"
   },
   "source": [
    "然后，你可以通过以下方式进行处理："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jyHzrtbSeART"
   },
   "outputs": [],
   "source": [
    "lstm_layer = layers.LSTM(64, stateful=True)\n",
    "for s in sub_sequences:\n",
    "  output = lstm_layer(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mv0-Al7veCDT"
   },
   "source": [
    "当你想要清除状态时，可以使用`layer.reset_states()`。\n",
    "\n",
    "注：在此设置中，给定批次中的样本i将被假定为前面批次样本的延续。这意味着**所有批次应包含相同数量的样本（批次大小）**。例如，如果一个批次包含`[sequence_A_from_t0_to_t100, sequence_B_from_t0_to_t100]`，则下一个批次应包含`[sequence_A_from_t101_to_t200, sequence_B_from_t101_to_t200]` 。\n",
    "\n",
    "下面是一个完整的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dAc30K8yeHp_"
   },
   "outputs": [],
   "source": [
    "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "\n",
    "lstm_layer = layers.LSTM(64, stateful=True)\n",
    "output = lstm_layer(paragraph1)\n",
    "output = lstm_layer(paragraph2)\n",
    "output = lstm_layer(paragraph3)\n",
    "\n",
    "# reset_states())会将缓存的状态重置为原始的initial_state。\n",
    "# 如果未提供initial_state，则默认情况下将使用零状态。\n",
    "lstm_layer.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NqetbjUdeIzg"
   },
   "source": [
    "### RNN状态重用\n",
    "\n",
    "RNN层的状态记录不包含在`layer.weights()`中。如果你想重用RNN层的状态，则可以通过**layer.states**检索状态值并将其用作新层的初始状态，就像函数式API（或子类化模型）中的`new_layer(inputs, initial_state=layer.states)`。\n",
    "\n",
    "请注意，在这种情况下可能不会使用`Sequential`模型，因为它仅支持具有单个输入和输出的层，初始状态的额外输入使得无法使用`Sequential`模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pin7y_D8eNUa"
   },
   "outputs": [],
   "source": [
    "paragraph1 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "paragraph2 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "paragraph3 = np.random.random((20, 10, 50)).astype(np.float32)\n",
    "\n",
    "lstm_layer = layers.LSTM(64, stateful=True)\n",
    "output = lstm_layer(paragraph1)\n",
    "output = lstm_layer(paragraph2)\n",
    "\n",
    "existing_state = lstm_layer.states\n",
    "\n",
    "new_lstm_layer = layers.LSTM(64)\n",
    "new_output = new_lstm_layer(paragraph3, initial_state=existing_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CjWms7uxePFJ"
   },
   "source": [
    "### 双向RNN\n",
    "\n",
    "RNN模型不仅可以从头到尾处理序列，而且可以反向处理序列。通常情况下，对于时间序列以外的序列（例如文本），双向RNN效果会更好。例如，要预测句子中的下一个单词，通常使单词具有上下文。\n",
    "\n",
    "Keras提供了一个简单的API，用于构建此类双向RNN：[`keras.layers.Bidirectional`]包装器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "lxR4H8u6eSJY",
    "outputId": "1895aa3a-c5fd-4c76-d5b4-a9e5b9b4909b"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "model.add(\n",
    "    layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10))\n",
    ")\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvQMrlp5eT-g"
   },
   "source": [
    "`Bidirectional`将复制传入的RNN层，并翻转新层的`go_backwards`字段，以便它能够处理输入的反序列。\n",
    "\n",
    "默认情况下，`Bidirectional` RNN的输出将是前向层输出和后向层输出的总和。如果你需要其他合并操作（如concatenation），请在`Bidirectional`的构造函数中更改`merge_mode`属性。有关`Bidirectional`更多详细信息，请检查[API文档](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Bidirectional/)。\n",
    "\n",
    "### 性能优化和CuDNN内核\n",
    "\n",
    "在TensorFlow 2.0中，内置的LSTM和GRU层已更新为在GPU可用时，默认使用CuDNN内核。通过此更改，先前的`keras.layers.CuDNNLSTM/CuDNNGRU`层已被弃用，你可以构建模型而不必关心模型所运行的硬件。\n",
    "\n",
    "由于CuDNN内核是根据某些假设构建的，因此，这意味着**如果你更改内置LSTM或GRU层的默认设置，则该层将无法使用CuDNN内核**。例如：\n",
    "\n",
    "+ 将`activation`方法从`tanh`更改为其他激活方法。\n",
    "+ 将`recurrent_activation`方法从`sigmoid`更改为其他方法。\n",
    "+ 设置`recurrent_dropout` > 0。\n",
    "+ 将`unroll`设置为True，这将强制LSTM/GRU把内部的`tf.while_loop`分解为`for`循环。\n",
    "+ 将`use_bias`设置为False。\n",
    "+ 当输入数据未严格进行右填充时，使用掩码（如果掩码对应于严格的右填充数据，则仍可以使用CuDNN，这是最常见的情况）。\n",
    "\n",
    "有关约束的详细列表，请参阅[LSTM](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/LSTM/)和[GRU](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/GRU/)层的文档。\n",
    "\n",
    "#### 在CuDNN内核可用时使用它\n",
    "让我们编写一个简单的LSTM模型来演示性能差异。\n",
    "\n",
    "我们将使用MNIST数字的行序列作为输入序列（将每个像素行作为时间步进行处理），并预测该数字的标签。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FLCVCVdleZ5P"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "# 每个MNIST图像批量都是一个形状为（batch_size，28，28）的张量\n",
    "# 每个输入序列的大小为（28，28）（将高度视为时间）\n",
    "input_dim = 28\n",
    "\n",
    "units = 64\n",
    "output_size = 10  # labels are from 0 to 9\n",
    "\n",
    "# 创建RNN模型\n",
    "def build_model(allow_cudnn_kernel=True):\n",
    "    # CuDNN仅在层级别时可用，而在单元级别时不可用。\n",
    "    # 这意味着`LSTM(units)`将使用CuDNN内\n",
    "    # 核，而RNN(LSTMCell(units))将在非CuDNN内核上运行。\n",
    "    if allow_cudnn_kernel:\n",
    "        # 具有默认选项的LSTM层使用CuDNN\n",
    "        lstm_layer = keras.layers.LSTM(units, input_shape=(None, input_dim))\n",
    "    else:\n",
    "        # 在RNN层中包装LSTMCell不会使用CuDNN\n",
    "        lstm_layer = keras.layers.RNN(\n",
    "            keras.layers.LSTMCell(units), input_shape=(None, input_dim)\n",
    "        )\n",
    "    model = keras.models.Sequential(\n",
    "        [\n",
    "            lstm_layer,\n",
    "            keras.layers.BatchNormalization(),\n",
    "            keras.layers.Dense(output_size),\n",
    "        ]\n",
    "    )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GKPlxXwhea_7"
   },
   "source": [
    "让我们加载MNIST数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "B30f7EkUecwH",
    "outputId": "432fdb7e-1073-46e9-a5d6-c22a092791eb"
   },
   "outputs": [],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "sample, sample_label = x_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-LXaQa1ped0c"
   },
   "source": [
    "让我们创建一个模型实例并对其进行训练。\n",
    "\n",
    "我们选择`sparse_categorical_crossentropy`作为模型的损失函数。模型的输出的形状为`[batch_size, 10]`。该模型的目标是一个整数向量，每个整数都在0到9的范围内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "uiYU_S2IefN5",
    "outputId": "cd80e197-a66b-41e3-cb96-3e261205ec1f"
   },
   "outputs": [],
   "source": [
    "model = build_model(allow_cudnn_kernel=True)\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RT2KKjUPegZa"
   },
   "source": [
    "现在，让我们与不使用CuDNN内核的模型进行比较："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "FTrGxDE1ehze",
    "outputId": "88f33c12-dbee-4208-94fb-e701aad06617"
   },
   "outputs": [],
   "source": [
    "noncudnn_model = build_model(allow_cudnn_kernel=False)\n",
    "noncudnn_model.set_weights(model.get_weights())\n",
    "noncudnn_model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=\"sgd\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "noncudnn_model.fit(\n",
    "    x_train, y_train, validation_data=(x_test, y_test), batch_size=batch_size, epochs=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hLYvSKmiei1Z"
   },
   "source": [
    "在安装了NVIDIA GPU和CuDNN的计算机上运行时，与使用常规TensorFlow内核的模型相比，使用CuDNN构建的模型的训练速度要快得多。\n",
    "\n",
    "启用了CuDNN的模型也可以用于仅有CPU的环境中进行预测，下面的[`tf.device`](https://tensorflow.google.cn/api_docs/python/tf/device)只是强制闲置设备。如果没有可用的GPU，默认情况下该模型将在CPU上运行。\n",
    "\n",
    "你完全不必关心正在运行的硬件，这不是很酷的一件事儿吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "oy3EQhcGekk2",
    "outputId": "102ac8d7-3789-48b9-f5f6-0d9f15df4a4e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with tf.device(\"CPU:0\"):\n",
    "    cpu_model = build_model(allow_cudnn_kernel=True)\n",
    "    cpu_model.set_weights(model.get_weights())\n",
    "    result = tf.argmax(cpu_model.predict_on_batch(tf.expand_dims(sample, 0)), axis=1)\n",
    "    print(\n",
    "        \"Predicted result is: %s, target result is: %s\" % (result.numpy(), sample_label)\n",
    "    )\n",
    "    plt.imshow(sample, cmap=plt.get_cmap(\"gray\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dnVRuchVemZa"
   },
   "source": [
    "### 具有列表/字典输入或嵌套输入的RNN\n",
    "\n",
    "嵌套结构使得实现者在单个时间步之内能够包括更多信息，例如，一个视频帧可以同时具有音频和视频输入。在这种情况下，数据形状可能是：\n",
    "\n",
    "`[batch, timestep, {\"video\": [height, width, channel], \"audio\": [frequency]}]`\n",
    "\n",
    "在另一个示例中，笔迹数据可以具有笔的当前位置的坐标x和y以及压力信息。因此，数据表示可以是：\n",
    "\n",
    "`[batch, timestep, {\"location\": [x, y], \"pressure\": [force]}]`\n",
    "\n",
    "以下代码提供了一个示例，展示如何构建接受此类结构化输入的自定义RNN单元。\n",
    "\n",
    "### 定义一个支持嵌套输入/输出的自定义单元\n",
    "\n",
    "有关编写自己的层的详细信息，请参见[通过子类化创建新的层和模型](https://tensorflow.google.cn/guide/keras/custom_layers_and_models/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P-51FNsceujq"
   },
   "outputs": [],
   "source": [
    "class NestedCell(keras.layers.Layer):\n",
    "    def __init__(self, unit_1, unit_2, unit_3, **kwargs):\n",
    "        self.unit_1 = unit_1\n",
    "        self.unit_2 = unit_2\n",
    "        self.unit_3 = unit_3\n",
    "        self.state_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
    "        self.output_size = [tf.TensorShape([unit_1]), tf.TensorShape([unit_2, unit_3])]\n",
    "        super(NestedCell, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shapes):\n",
    "        # input_shape应该包含2个条目，[（batch，i1），（batch，i2，i3）]\n",
    "        i1 = input_shapes[0][1]\n",
    "        i2 = input_shapes[1][1]\n",
    "        i3 = input_shapes[1][2]\n",
    "\n",
    "        self.kernel_1 = self.add_weight(\n",
    "            shape=(i1, self.unit_1), initializer=\"uniform\", name=\"kernel_1\"\n",
    "        )\n",
    "        self.kernel_2_3 = self.add_weight(\n",
    "            shape=(i2, i3, self.unit_2, self.unit_3),\n",
    "            initializer=\"uniform\",\n",
    "            name=\"kernel_2_3\",\n",
    "        )\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        # 输入的形状应为[(batch, input_1), (batch, input_2, input_3)]\n",
    "        # 状态的形状应为[(batch, unit_1), (batch, unit_2, unit_3)]\n",
    "        input_1, input_2 = tf.nest.flatten(inputs)\n",
    "        s1, s2 = states\n",
    "\n",
    "        output_1 = tf.matmul(input_1, self.kernel_1)\n",
    "        output_2_3 = tf.einsum(\"bij,ijkl->bkl\", input_2, self.kernel_2_3)\n",
    "        state_1 = s1 + output_1\n",
    "        state_2_3 = s2 + output_2_3\n",
    "\n",
    "        output = (output_1, output_2_3)\n",
    "        new_states = (state_1, state_2_3)\n",
    "\n",
    "        return output, new_states\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\"unit_1\": self.unit_1, \"unit_2\": unit_2, \"unit_3\": self.unit_3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AXyK7nC1ev0i"
   },
   "source": [
    "#### 使用嵌套的输入/输出构建RNN模型\n",
    "\n",
    "让我们使用[`keras.layers.RNN`](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/RNN)层和我们的自定义单元构建一个Keras模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a8Ytj4riezGs"
   },
   "outputs": [],
   "source": [
    "unit_1 = 10\n",
    "unit_2 = 20\n",
    "unit_3 = 30\n",
    "\n",
    "i1 = 32\n",
    "i2 = 64\n",
    "i3 = 32\n",
    "batch_size = 64\n",
    "num_batches = 10\n",
    "timestep = 50\n",
    "\n",
    "cell = NestedCell(unit_1, unit_2, unit_3)\n",
    "rnn = keras.layers.RNN(cell)\n",
    "\n",
    "input_1 = keras.Input((None, i1))\n",
    "input_2 = keras.Input((None, i2, i3))\n",
    "\n",
    "outputs = rnn((input_1, input_2))\n",
    "\n",
    "model = keras.models.Model([input_1, input_2], outputs)\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "83uH2Iiae0ZY"
   },
   "source": [
    "#### 使用随机生成的数据训练模型\n",
    "\n",
    "由于此模型没有好的候选数据集，因此我们使用随机的Numpy数据进行演示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "0_w9NJxre4Bj",
    "outputId": "7c8c0972-d082-46e0-c603-e01a7b0fe45f"
   },
   "outputs": [],
   "source": [
    "input_1_data = np.random.random((batch_size * num_batches, timestep, i1))\n",
    "input_2_data = np.random.random((batch_size * num_batches, timestep, i2, i3))\n",
    "target_1_data = np.random.random((batch_size * num_batches, unit_1))\n",
    "target_2_data = np.random.random((batch_size * num_batches, unit_2, unit_3))\n",
    "input_data = [input_1_data, input_2_data]\n",
    "target_data = [target_1_data, target_2_data]\n",
    "\n",
    "model.fit(input_data, target_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBSoun_Ve5Pu"
   },
   "source": [
    "使用[`keras.layers.RNN`]层，只需要为序列中的单个步骤定义数学逻辑，而[`keras.layers.RNN`]层将为你处理序列迭代，这是一种快速原型化新型RNN（例如LSTM变体）的强大方法。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMjvIH21FGTjE5ab8/z9wwJ",
   "collapsed_sections": [],
   "name": "Keras循环神经网络.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py37tf2",
   "language": "python",
   "name": "py37tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
